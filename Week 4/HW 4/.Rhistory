knitr::opts_chunk$set(echo = TRUE)
# Install Packages
# install.packages("factoextra")
# Load Packages
library(arules)
library(arulesViz)
library(ggplot2)
library(tidyverse)
library(psych)
library(cluster)
library(factoextra)
library(dendextend)
library(dplyr)
# Load the data
federalistPapers <- read.csv("/Users/timtieng/Library/CloudStorage/OneDrive-Personal/Desktop/Masters in Applied Data Science/IST-707 Machine Learning/Week 4/HW 4/fedPapers85.csv", na.strings = c(""))
# describe/summarize
str(federalistPapers)
describe(federalistPapers)
# Observations:
# this R output provides preliminary information about the data set, including its size, data types, and summary statistics for each word's frequency across the documents.
# This information can be helpful for analysts to understand the structure and content of the dataset and to perform further data analysis and modeling tasks.
# Preliminary look of raw data
View(federalistPapers)
# Breakdown of how many papers were written by a specific author
# Total of 5 categories, where HM == Hamilton/Madison Co-Authored paper
# Dispt = Unknown author
table(federalistPapers[,1])
# Visualize author breakdown
result<- table(federalistPapers[,1])
authorsDF <- as.data.frame(result)
# create a vector of colors
colors <- c("#1f78b4", "#33a02c", "#e31a1c", "#ff7f00", "#6a3d9a")
pie_chart <- ggplot(authorsDF, aes(x = "", y = Freq, fill=factor(Var1))) +
geom_bar(stat = "identity", color = "black") +
coord_polar("y", start = 0) +
labs(fill = "Author") +
ggtitle("Author Breakdown - Pie Chart")+
theme_void() +
scale_fill_manual(values = colors)
pie_chart
# Check for null values
sum(is.na(federalistPapers))
# Create dataframes for each author via subsetting
hamiltonPapers <- federalistPapers[federalistPapers$author == 'Hamilton']
hamMadPapers <-federalistPapers[federalistPapers$author == 'HM']
jayPapers <- federalistPapers[federalistPapers$author == 'Jay']
madisonPapers <- federalistPapers[federalistPapers$author == 'Madison',]
disputedPapers <- federalistPapers[federalistPapers$author == 'dispt']
# inspect
str(hamiltonPapers)
str(hamMadPapers)
str(jayPapers)
str(madisonPapers)
str(disputedPapers)
# Create a function that finds the average tfidf of a word for a specific author
createWordMean <- function(x){
y <- ncol(x) # Calculate the number of columns to use as length for iteration
x <- colMeans(x[,3:y]) # calculates the column means for columns from the third column (3) to the last column
newVector1 <- c() # Empty array
for (i in 1:length(x)) {
newVector1[i] <-x[[i]]
}
print(newVector1)
}
str(hamMadPapers)
# Call the function on each author array
hamiltonVector <- createWordMean(hamiltonPapers)
hmVector <- createWordMean(hamMadPapers)
# Create dataframes for each author via subsetting
hamiltonPapers <- federalistPapers[federalistPapers$author == 'Hamilton']
hmPapers <-federalistPapers[federalistPapers$author == 'HM']
jayPapers <- federalistPapers[federalistPapers$author == 'Jay']
madisonPapers <- federalistPapers[federalistPapers$author == 'Madison',]
disputedPapers <- federalistPapers[federalistPapers$author == 'dispt']
# inspect
str(hamiltonPapers)
str(hmPapers)
str(jayPapers)
str(madisonPapers)
str(disputedPapers)
# Create a function that finds the average tfidf of a word for a specific author
createWordMean <- function(x){
y <- ncol(x) # Calculate the number of columns to use as length for iteration
x <- colMeans(x[,3:y]) # calculates the column means for columns from the third column (3) to the last column
newVector1 <- c() # Empty array
for (i in 1:length(x)) {
newVector1[i] <-x[[i]]
}
print(newVector1)
}
str(hamMadPapers)
# Call the function on each author array
hamiltonVector <- createWordMean(hamiltonPapers)
hmVector <- createWordMean(hmPapers)
# Call the function on each author array
hamiltonVector <- createWordMean(hamiltonPapers)
hmVector <- createWordMean(hmPapers)
# Call the function on each author array
hamiltonVector <- createWordMean(hamiltonPapers)
# hmVector <- createWordMean(hmPapers)
jayVector <- createWordMean(jayPapers)
madisonVector <- createWordMean(madisonPapers)
# Print
hamiltonVector
# hmVector
jayVector
madisonVector
# Create a function that finds the average tfidf of a word for a specific author
createWordMean <- function(x){
y <- ncol(x) # Calculate the number of columns to use as length for iteration
x <- colMeans(x[,3:y]) # calculates the column means for columns from the third column (3) to the last column for every row
newVector1 <- c() # Empty array
for (i in 1:length(x)) {
newVector1[i] <-x[[i]]
}
print(newVector1)
}
str(hmPapers)
str(hmPapers)
tail(hmPapers)
# Call the function on each author array
hamiltonVector <- createWordMean(hamiltonPapers)
hmVector <- createWordMean(hmPapers)
# Call the function on each author array
hamiltonVector <- createWordMean(hamiltonPapers)
hmVector <- createWordMean(hmPapers)
# Create dataframes for each author via subsetting
hamiltonPapers <- federalistPapers[federalistPapers$author == 'Hamilton']
hmPapers <-data.frame(federalistPapers[federalistPapers$author == 'HM'])
jayPapers <- federalistPapers[federalistPapers$author == 'Jay']
madisonPapers <- federalistPapers[federalistPapers$author == 'Madison',]
disputedPapers <- federalistPapers[federalistPapers$author == 'dispt']
# inspect
str(hamiltonPapers)
str(hmPapers)
str(jayPapers)
str(madisonPapers)
str(disputedPapers)
# Call the function on each author array
hamiltonVector <- createWordMean(hamiltonPapers)
hmVector <- createWordMean(hmPapers)
# Create a function that finds the average tfidf of a word for a specific author
createWordMean <- function(x){
y <- ncol(x) # Calculate the number of columns to use as length for iteration
x <- rowMeans(x[,3:y]) # calculates the column means for columns from the third column (3) to the last column for every row
newVector1 <- c() # Empty array
for (i in 1:length(x)) {
newVector1[i] <-x[[i]]
}
print(newVector1)
}
# Call the function on each author array
hamiltonVector <- createWordMean(hamiltonPapers)
hmVector <- createWordMean(hmPapers)
# Create a function that finds the average tfidf of a word for a specific author
createWordMean <- function(x){
y <- ncol(x) # Calculate the number of columns to use as length for iteration
x <- colMeans(x[,3:y]) # calculates the column means for columns from the third column (3) to the last column for every row
newVector1 <- c() # Empty array
for (i in 1:length(x)) {
newVector1[i] <-x[[i]]
}
print(newVector1)
}
# Call the function on each author array
hamiltonVector <- createWordMean(hamiltonPapers)
hmVector <- createWordMean(hmPapers)
# Call the function on each author array
hamiltonVector <- createWordMean(hamiltonPapers)
# hmVector <- createWordMean(hmPapers)
jayVector <- createWordMean(jayPapers)
madisonVector <- createWordMean(madisonPapers)
# Print
#hamiltonVector
#hmVector
#jayVector
# madisonVector
columns <- colnames(hamiltonPapers)
# Use results to create a new dataframe for use
newDataFrame1 <- hamiltonPapers
newDataFrame1 <- data.frame(rbind(hamiltonVector,jayVector,madisonVector))
columns <- colnames(hamiltonPapers)
# Use results to create a new dataframe for use
newDataFrame1 <- hamiltonPapers
newDataFrame1 <- data.frame(rbind(hamiltonVector,jayVector,madisonVector))
# Name the columns
colnames(newDataFrame1) <- columns[3:length(columns)]
newDataFrame1[,1:3]
head(hamiltonPapers)
str(hmPapers)
tail(hmPapers)
# Create dataframes for each author via subsetting
hamiltonPapers <- federalistPapers[federalistPapers$author == 'Hamilton',]
hmPapers <-data.frame(federalistPapers[federalistPapers$author == 'HM',])
jayPapers <- federalistPapers[federalistPapers$author == 'Jay',]
madisonPapers <- federalistPapers[federalistPapers$author == 'Madison',]
disputedPapers <- federalistPapers[federalistPapers$author == 'dispt',]
# inspect
str(hamiltonPapers)
str(hmPapers)
str(jayPapers)
str(madisonPapers)
str(disputedPapers)
# Create a function that finds the average tfidf of a word for a specific author
createWordMean <- function(x){
y <- ncol(x) # Calculate the number of columns to use as length for iteration
x <- colMeans(x[,3:y]) # calculates the column means for columns from the third column (3) to the last column for every row
newVector1 <- c() # Empty array
for (i in 1:length(x)) {
newVector1[i] <-x[[i]]
}
print(newVector1)
}
# Call the function on each author array
hamiltonVector <- createWordMean(hamiltonPapers)
hmVector <- createWordMean(hmPapers)
jayVector <- createWordMean(jayPapers)
madisonVector <- createWordMean(madisonPapers)
# Print
#hamiltonVector
#hmVector
#jayVector
# madisonVector
# Call the function on each author array
hamiltonVector <- createWordMean(hamiltonPapers)
hmVector <- createWordMean(hmPapers)
jayVector <- createWordMean(jayPapers)
madisonVector <- createWordMean(madisonPapers)
# Print
hamiltonVector
hmVector
jayVector
madisonVector
columns <- colnames(hamiltonPapers)
# Use results to create a new dataframe for use
newDataFrame1 <- hamiltonPapers
newDataFrame1 <- data.frame(rbind(hamiltonVector,hmVector,jayVector,madisonVector))
# Name the columns
colnames(newDataFrame1) <- columns[3:length(columns)]
newDataFrame1[,1:3]
# Find the vsariance of each word
wordVariance <- sapply(newDataFrame1, var)
# Order it and plot it
orderedVariance <- sort(wordVariance, decreasing = TRUE)
plot(orderedVariance)
goodWords <- data.frame(ordered[1:length(ordered) * 3/5])
goodWords <- data.frame(ordered[1:length(ordered) * 2/5])
goodWords <- data.frame(ordered[1:(length(ordered) * 2/5)])
goodWords <- data.frame(ordered[1:(length(ordered)* 2/5)])
goodWords <- data.frame(ordered[1:(length(ordered)* 2/5)])
goodWords <- data.frame(ordered[1:(length(ordered)* 2/5)])
# K-Means
# Remove author names from dataset for clustering purposes
federalistPapers_KMeans <-federalistPapers[,2:72]
# Make the file names the row names. Need a dataframe of numerical values for k-means
rownames(federalistPapers_KMeans) <- federalistPapers_KMeans[,1]
federalistPapers_KMeans[,1] <- NULL
View(federalistPapers_KMeans)
# Determine "Optimal" number of clusters
fviz_nbclust(federalistPapers, FUN=hcut,method = "wss")
fviz_nbclust(federalistPapers, FUN =hcut, method = "silhouette")
# Set seed for fixed random seed
set.seed(20)
# run k-means
Clusters <- kmeans(federalistPapers_KMeans, 6)
federalistPapers_KMeans$Clusters <- as.factor(Clusters$cluster)
str(Clusters)
Clusters$centers
# Add Clusters to the Dataframe
federalistPapers_KMeans2 <- federalistPapers
federalistPapers_KMeans2$Clusters <- as.factor(Clusters$cluster)
# Plot KMeans 1
clusplot(federalistPapers_KMeans, federalistPapers_KMeans$Clusters, color= TRUE, shade=TRUE, labels=0, lines=0)
# Plot KMeans2
ggplot(data=federalistPapers_KMeans2, aes(x=author, fill=Clusters))+
geom_bar(stat="count")+
labs(title= "K = ?") +
theme(plot.title = element_text(hjust = 0.5), text=element_text(size = 15))
# Hierarchical Clustering Algorithms (HAC)
# Remove author names from data set
federalistPapers_HAC <- federalistPapers[,c(2:72)]
# Make the file names the row names. Need a dataframe of numerical values for HAC
rownames(federalistPapers_HAC) <- federalistPapers_HAC[,1]
federalistPapers_HAC[,1] <- NULL
View(federalistPapers_HAC)
# Calculate distance in a variety of ways
distance  <- dist(federalistPapers_HAC, method = "euclidean")
distance2 <- dist(federalistPapers_HAC, method = "maximum")
distance3 <- dist(federalistPapers_HAC, method = "manhattan")
distance4 <- dist(federalistPapers_HAC, method = "canberra")
distance5 <- dist(federalistPapers_HAC, method = "binary")
distance6 <- dist(federalistPapers_HAC, method = "minkowski")
# Below we display the results of HAC. The boxes indicate clusters.
HAC <- hclust(distance6, method="complete")
plot(HAC, cex=0.6, hang=-1)
rect.hclust(HAC, k =6, border=2:5)
HAC2 <- hclust(distance3, method="complete")
plot(HAC2, cex=0.6, hang=-1)
rect.hclust(HAC2, k =5, border=2:5)
# Below we display the results of HAC. The boxes indicate clusters.
HAC <- hclust(distance, method="complete")
plot(HAC, cex=0.6, hang=-1)
rect.hclust(HAC, k =6, border=2:5)
HAC2 <- hclust(distance2, method="complete")
plot(HAC, cex=0.6, hang=-1)
rect.hclust(HAC, k =6, border=2:5)
HAC3 <- hclust(distance3, method="complete")
plot(HAC2, cex=0.6, hang=-1)
rect.hclust(HAC2, k =5, border=2:5)
HAC4 <- hclust(distance4, method="complete")
plot(HAC2, cex=0.6, hang=-1)
rect.hclust(HAC2, k =5, border=2:5)
HAC5 <- hclust(distance5, method="complete")
plot(HAC2, cex=0.6, hang=-1)
rect.hclust(HAC2, k =5, border=2:5)
HAC6 <- hclust(distance6, method="complete")
plot(HAC2, cex=0.6, hang=-1)
rect.hclust(HAC2, k =5, border=2:5)
# Below we display the results of HAC. The boxes indicate clusters.
HAC <- hclust(distance, method="complete")
plot(HAC, cex=0.6, hang=-1)
rect.hclust(HAC, k =2, border=2:5)
HAC2 <- hclust(distance2, method="complete")
plot(HAC, cex=0.6, hang=-1)
rect.hclust(HAC, k =2, border=2:5)
HAC3 <- hclust(distance3, method="complete")
plot(HAC2, cex=0.6, hang=-1)
rect.hclust(HAC2, k =2, border=2:5)
HAC4 <- hclust(distance4, method="complete")
plot(HAC2, cex=0.6, hang=-1)
rect.hclust(HAC2, k =2, border=2:5)
HAC5 <- hclust(distance5, method="complete")
plot(HAC2, cex=0.6, hang=-1)
rect.hclust(HAC2, k =2, border=2:5)
HAC6 <- hclust(distance6, method="complete")
plot(HAC2, cex=0.6, hang=-1)
rect.hclust(HAC2, k =2, border=2:5)
knitr::opts_chunk$set(echo = TRUE)
# Install Packages
# install.packages("factoextra")
# Load Packages
library(arules)
library(arulesViz)
library(ggplot2)
library(tidyverse)
library(psych)
library(cluster)
library(factoextra)
library(dendextend)
library(dplyr)
# Load the data
federalistPapers <- read.csv("/Users/timtieng/Library/CloudStorage/OneDrive-Personal/Desktop/Masters in Applied Data Science/IST-707 Machine Learning/Week 4/HW 4/fedPapers85.csv", na.strings = c(""))
# describe/summarize
str(federalistPapers)
describe(federalistPapers)
# Observations:
# this R output provides preliminary information about the data set, including its size, data types, and summary statistics for each word's frequency across the documents.
# This information can be helpful for analysts to understand the structure and content of the dataset and to perform further data analysis and modeling tasks.
# Preliminary look of raw data
# View(federalistPapers)
# Breakdown of how many papers were written by a specific author
# Total of 5 categories, where HM == Hamilton/Madison Co-Authored paper
# Dispt = Unknown author
table(federalistPapers[,1])
# Visualize author breakdown
result<- table(federalistPapers[,1])
authorsDF <- as.data.frame(result)
# create a vector of colors
colors <- c("#1f78b4", "#33a02c", "#e31a1c", "#ff7f00", "#6a3d9a")
pie_chart <- ggplot(authorsDF, aes(x = "", y = Freq, fill=factor(Var1))) +
geom_bar(stat = "identity", color = "black") +
coord_polar("y", start = 0) +
labs(fill = "Author") +
ggtitle("Author Breakdown - Pie Chart")+
theme_void() +
scale_fill_manual(values = colors)
pie_chart
# Check for null values
sum(is.na(federalistPapers))
# Create dataframes for each author via subsetting
hamiltonPapers <- federalistPapers[federalistPapers$author == 'Hamilton',]
hmPapers <-data.frame(federalistPapers[federalistPapers$author == 'HM',])
jayPapers <- federalistPapers[federalistPapers$author == 'Jay',]
madisonPapers <- federalistPapers[federalistPapers$author == 'Madison',]
disputedPapers <- federalistPapers[federalistPapers$author == 'dispt',]
# inspect
str(hamiltonPapers)
str(hmPapers)
str(jayPapers)
str(madisonPapers)
str(disputedPapers)
# Create a function that finds the average  of a word for a specific author
createWordMean <- function(x){
y <- ncol(x) # Calculate the number of columns to use as length for iteration
x <- colMeans(x[,3:y]) # calculates the column means for columns from the third column (3) to the last column for every row
newVector1 <- c() # Empty array
for (i in 1:length(x)) {
newVector1[i] <-x[[i]]
}
print(newVector1)
}
# Call the function on each author array
hamiltonVector <- createWordMean(hamiltonPapers)
hmVector <- createWordMean(hmPapers)
jayVector <- createWordMean(jayPapers)
madisonVector <- createWordMean(madisonPapers)
# Print
table(hamiltonVector)
table(hmVector)
table(jayVector)
table(madisonVector)
columns <- colnames(hamiltonPapers)
# Use results to create a new dataframe for use
newDataFrame1 <- hamiltonPapers
newDataFrame1 <- data.frame(rbind(hamiltonVector,hmVector,jayVector,madisonVector))
# Name the columns
colnames(newDataFrame1) <- columns[3:length(columns)]
newDataFrame1[,1:3]
# Find the variance of each word
wordVariance <- sapply(newDataFrame1, var)
# Order it and plot it
orderedVariance <- sort(wordVariance, decreasing = TRUE)
plot(orderedVariance)
# Observations
# Higher variance features can have a more significant impact on the clustering process in K-means. They might contribute more to the calculation of cluster centroids,
# potentially leading to more distinct and well-separated clusters. By analyzing the ordered variances, we can decide whether to include all features or select a
# subset of high-variance features for improved clustering results.
# Understanding the variance distribution of features can assist in making informed decisions about feature selection,
# improving the quality of clustering, and potentially revealing underlying structures in the data.
# K-Means
# Remove author names from dataset for clustering purposes
federalistPapers_KMeans <-federalistPapers[,2:72]
# Make the file names the row names. Need a dataframe of numerical values for k-means
rownames(federalistPapers_KMeans) <- federalistPapers_KMeans[,1]
federalistPapers_KMeans[,1] <- NULL
View(federalistPapers_KMeans)
# Determine "Optimal" number of clusters
fviz_nbclust(federalistPapers, FUN=hcut,method = "wss")
fviz_nbclust(federalistPapers, FUN =hcut, method = "silhouette")
# Set seed for fixed random seed
set.seed(20)
# run k-means
Clusters <- kmeans(federalistPapers_KMeans, 5)
federalistPapers_KMeans$Clusters <- as.factor(Clusters$cluster)
str(Clusters)
Clusters$centers
# Add Clusters to the Dataframe
federalistPapers_KMeans2 <- federalistPapers
federalistPapers_KMeans2$Clusters <- as.factor(Clusters$cluster)
# Plot KMeans 1
clusplot(federalistPapers_KMeans, federalistPapers_KMeans$Clusters, color= TRUE, shade=TRUE, labels=0, lines=0)
# Plot KMeans2
ggplot(data=federalistPapers_KMeans2, aes(x=author, fill=Clusters))+
geom_bar(stat="count")+
labs(title= "K = ?") +
theme(plot.title = element_text(hjust = 0.5), text=element_text(size = 15))
# Hierarchical Clustering Algorithms (HAC)
# Remove author names from data set
federalistPapers_HAC <- federalistPapers[,c(2:72)]
# Make the file names the row names. Need a data frame of numerical values for HAC
rownames(federalistPapers_HAC) <- federalistPapers_HAC[,1]
federalistPapers_HAC[,1] <- NULL
View(federalistPapers_HAC)
# Calculate distance in a variety of ways
distance  <- dist(federalistPapers_HAC, method = "euclidean")
distance2 <- dist(federalistPapers_HAC, method = "maximum")
distance3 <- dist(federalistPapers_HAC, method = "manhattan")
distance4 <- dist(federalistPapers_HAC, method = "canberra")
distance5 <- dist(federalistPapers_HAC, method = "binary")
distance6 <- dist(federalistPapers_HAC, method = "minkowski")
# Below we display the results of HAC. The boxes indicate clusters.
HAC <- hclust(distance, method="complete")
plot(HAC, cex=0.6, hang=-1)
rect.hclust(HAC, k =5, border=2:5)
HAC2 <- hclust(distance2, method="complete")
plot(HAC, cex=0.6, hang=-1)
rect.hclust(HAC, k =5, border=2:5)
HAC3 <- hclust(distance3, method="complete")
plot(HAC2, cex=0.6, hang=-1)
rect.hclust(HAC2, k =5, border=2:5)
HAC4 <- hclust(distance4, method="complete")
plot(HAC2, cex=0.6, hang=-1)
rect.hclust(HAC2, k =5, border=2:5)
HAC5 <- hclust(distance5, method="complete")
plot(HAC2, cex=0.6, hang=-1)
rect.hclust(HAC2, k =5, border=2:5)
HAC6 <- hclust(distance6, method="complete")
plot(HAC2, cex=0.6, hang=-1)
rect.hclust(HAC2, k =5, border=2:5)
