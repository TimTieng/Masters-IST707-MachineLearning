for (k in 1:kfolds) {
new_test <- svm_trainset[holdout[[k]], ]
new_train <- svm_trainset[-holdout[[k]], ]
new_test_no_label <- new_test[-c(1)]
new_test_just_label <- new_test[c(1)]
test_model <- svm(label ~ ., new_train, na.action=na.pass)
pred <- predict(test_model, new_test_no_label, type=c("class"))
all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
# Binarizing preprocessed SVM trainset
binarized_svm_trainset <- svm_trainset
for (col in colnames(binarized_svm_trainset)) {
if (col != "label") {
binarized_svm_trainset[, c(col)] <- ifelse(binarized_svm_trainset[, c(col)] > 131, 1, 0)
}
}
for (col in colnames(binarized_svm_trainset)) {
if (col != "label") {
binarized_svm_trainset[, c(col)] <- as.factor(binarized_svm_trainset[, c(col)])
}
}
cols_to_remove = c()
for (col in colnames(binarized_svm_trainset)) {
if (col != "label") {
if (length(unique(binarized_svm_trainset[, c(col)])) == 1) {
cols_to_remove <- c(cols_to_remove, col)
}
}
}
binarized_svm_trainset <- binarized_svm_trainset[-which(colnames(binarized_svm_trainset) %in% cols_to_remove)]
# Testing SVM on new data
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds) {
new_test <- binarized_svm_trainset[holdout[[k]], ]
new_train <- binarized_svm_trainset[-holdout[[k]], ]
new_test_no_label <- new_test[-c(1)]
new_test_just_label <- new_test[c(1)]
test_model <- svm(label ~ ., new_train, na.action=na.pass)
pred <- predict(test_model, new_test_no_label, type=c("class"))
all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
# Did accurracy improve here? Which sets of parameters seem to work best for SVMs on this data set??? lets try some more.
# Polynomial Kernel
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds) {
new_test <- binarized_svm_trainset[holdout[[k]], ]
new_train <- binarized_svm_trainset[-holdout[[k]], ]
new_test_no_label <- new_test[-c(1)]
new_test_just_label <- new_test[c(1)]
test_model <- svm(label ~ ., new_train, kernel="polynomial", na.action=na.pass)
pred <- predict(test_model, new_test_no_label, type=c("class"))
all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
# Radial Kernel
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds) {
new_test <- binarized_svm_trainset[holdout[[k]], ]
new_train <- binarized_svm_trainset[-holdout[[k]], ]
new_test_no_label <- new_test[-c(1)]
new_test_just_label <- new_test[c(1)]
test_model <- svm(label ~ ., new_train, kernel="radial", na.action=na.pass)
pred <- predict(test_model, new_test_no_label, type=c("class"))
all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
# sigmoid Kernal
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds) {
new_test <- binarized_svm_trainset[holdout[[k]], ]
new_train <- binarized_svm_trainset[-holdout[[k]], ]
new_test_no_label <- new_test[-c(1)]
new_test_just_label <- new_test[c(1)]
test_model <- svm(label ~ ., new_train, kernel="sigmoid", na.action=na.pass)
pred <- predict(test_model, new_test_no_label, type=c("class"))
all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
# sigmoid Kernal
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds) {
new_test <- binarized_svm_trainset[holdout[[k]], ]
new_train <- binarized_svm_trainset[-holdout[[k]], ]
new_test_no_label <- new_test[-c(1)]
new_test_just_label <- new_test[c(1)]
test_model <- svm(label ~ ., new_train, kernel="varyingcost", na.action=na.pass)
pred <- predict(test_model, new_test_no_label, type=c("class"))
all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
# sigmoid Kernal
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds) {
new_test <- binarized_svm_trainset[holdout[[k]], ]
new_train <- binarized_svm_trainset[-holdout[[k]], ]
new_test_no_label <- new_test[-c(1)]
new_test_just_label <- new_test[c(1)]
test_model <- svm(label ~ ., new_train, kernel="sigmoid", na.action=na.pass)
pred <- predict(test_model, new_test_no_label, type=c("class"))
all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
# sigmoid Kernal
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds) {
new_test <- binarized_svm_trainset[holdout[[k]], ]
new_train <- binarized_svm_trainset[-holdout[[k]], ]
new_test_no_label <- new_test[-c(1)]
new_test_just_label <- new_test[c(1)]
test_model <- svm(label ~ ., new_train, kernel="cost", na.action=na.pass)
pred <- predict(test_model, new_test_no_label, type=c("class"))
all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
# Random Forest
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds) {
new_test <- trainset[holdout[[k]], ]
new_train <- trainset[-holdout[[k]], ]
new_test_no_label <- new_test[-c(1)]
new_test_just_label <- new_test[c(1)]
test_model <- randomForest(label ~ ., new_train, na.action=na.pass)
pred <- predict(test_model, new_test_no_label, type=c("class"))
all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
# Binarized Data
prev_result <- 0
best_result <- 0
best_number_trees <-0
for (trees in 5:15) {
if (trees %% 5 == 0) {
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds) {
new_test <- trainset[holdout[[k]], ]
new_train <- trainset[-holdout[[k]], ]
new_test_no_label <- new_test[-c(1)]
new_test_just_label <- new_test[c(1)]
test_model <- randomForest(label ~ ., new_train, replace=TRUE, na.action=na.pass)
pred <- predict(test_model, new_test_no_label, type=c("class"))
all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
#table(all_results$orig, all_results$pred)
new_result <- get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
if (new_result > prev_result) {
prev_result <- new_result
} else {
best_number_trees <- trees
best_result <- new_result
break
}
}
}
paste("Best Number of Trees:", best_number_trees, "- Best Result:", best_result, sep=" ")
table(all_results$orig, all_results$pred)
knitr::opts_chunk$set(echo = TRUE)
#install.packages("e1071")
#install.packages("naivebayes")
#install.packages("dplyr")
#install.packages("caret")
# install.packages("ggplot2")
#install.packages("rpart")
#install.packages("sqldf")
#install.packages("class")
#install.packages("randomForest")
library(e1071)
library(naivebayes)
library(dplyr)
library(ggplot2)
library(caret)
library(rpart)
library(rpart.plot)
library(sqldf)
library(class)
library(randomForest)
#First load the training data in csv format, and then convert "label" to nominal variable.
filename <-"/Users/timtieng/Library/CloudStorage/OneDrive-Personal/Desktop/Masters in Applied Data Science/IST-707 Machine Learning/Homework 6 and 7/digit-recognizer/train.csv"
DigitTotalDF <- read.csv(filename, header = TRUE, stringsAsFactors = TRUE)
DigitTotalDF$label<-as.factor(DigitTotalDF$label)
dim(DigitTotalDF)
#head(DigitTotalDF)
#Create a random sample of n% of train data set
#First load the training data in csv format, and then convert "label" to nominal variable.
filename <-"/Users/timtieng/Library/CloudStorage/OneDrive-Personal/Desktop/Masters in Applied Data Science/IST-707 Machine Learning/Homework 6 and 7/digit-recognizer/train.csv"
DigitTotalDF <- read.csv(filename, header = TRUE, stringsAsFactors = TRUE)
DigitTotalDF$label<-as.factor(DigitTotalDF$label)
dim(DigitTotalDF)
head(DigitTotalDF)
#Create a random sample of n% of train data set
# install.packages("FactoMineR")
# install.packages("rlang")
library(FactoMineR)
pca_digits = PCA(t(select(DigitTotalDF,-label)))
DigitTotalDF =   data.frame(DigitTotalDF$label,pca_digits$var$coord)
percent <- .25
set.seed(275)
DigitSplit <- sample(nrow(DigitTotalDF),nrow(DigitTotalDF)*percent)
DigitDF <- DigitTotalDF[DigitSplit,]
dim(DigitDF)
#(head(DigitDF))
#(str(DigitDF))
(nrow(DigitDF))
filename <-"/Users/timtieng/Library/CloudStorage/OneDrive-Personal/Desktop/Masters in Applied Data Science/IST-707 Machine Learning/Homework 6 and 7/digit-recognizer/test.csv"
TestTotalDF <- read.csv(filename, header = TRUE, stringsAsFactors = TRUE)
#TestTotalDF$label<-as.factor(TestTotalDF$label)
# Wont use test data, instead crossvalidation on train.
dim(TestTotalDF)
#(head(TestTotalDF))
# Create k-folds for k-fold cross validation
## Number of observations
N <- nrow(DigitDF)
## Number of desired splits
kfolds <- 10
## Generate indices of holdout observations
## Note if N is not a multiple of folds you will get a warning, but is OK.
holdout <- split(sample(1:N), 1:kfolds)
#head(holdout)
# Create k-folds for k-fold cross validation
## Number of observations
N <- nrow(DigitDF)
## Number of desired splits
kfolds <- 10
## Generate indices of holdout observations
## Note if N is not a multiple of folds you will get a warning, but is OK.
holdout <- split(sample(1:N), 1:kfolds)
head(holdout)
#Run training and Testing for each of the k-folds
AllResults<-list()
AllLabels<-list()
for (k in 1:kfolds){
DigitDF_Test <- DigitDF[holdout[[k]], ]
DigitDF_Train=DigitDF[-holdout[[k]], ]
## View the created Test and Train sets
#(head(DigitDF_Train))
#(table(DigitDF_Test$DigitTotalDF.label))
## Make sure you take the labels out of the testing data
#
DigitDF_Test_noLabel<-DigitDF_Test[-c(1)]
DigitDF_Test_justLabel<-DigitDF_Test$DigitTotalDF.label
#(head(DigitDF_Test_noLabel))
#### Naive Bayes prediction ussing e1071 package
#Naive Bayes Train model
train_naibayes<-naiveBayes(DigitTotalDF.label~., data=DigitDF_Train, na.action = na.pass)
#train_naibayes
#summary(train_naibayes)
#Naive Bayes model Prediction
nb_Pred <- predict(train_naibayes, DigitDF_Test_noLabel)
#nb_Pred
#Testing accurancy of naive bayes model with Kaggle train data sub set
(confusionMatrix(nb_Pred, DigitDF_Test$DigitTotalDF.label))
# Accumulate results from each fold, if you like
AllResults<- c(AllResults,nb_Pred)
AllLabels<- c(AllLabels, DigitDF_Test_justLabel)
##Visualize
plot(nb_Pred, ylab = "Density", main = "Naive Bayes Plot")
}
### end crossvalidation -- present results for all folds
(table(unlist(AllResults),unlist(AllLabels)))
#########Testing with Kaggle sample#########
#TestTotalDF_noLabel<-TestTotalDF[-c(1)]
#(head(TestTotalDF_noLabel))
#nb_Pred <- predict(train_naibayes, TestTotalDF_noLabel)
#nb_Pred
### Export naive Bayesbest result and run through Kaggle
#nbTestPred <- predict(train_naibayes,TestTotalDF, type = 'class')
#nbTestPred <- data.frame(nbTestPred)
#colnames(nbTestPred)[1] <- 'Label'
#nbTestPred$ImageId <- 1:nrow(nbTestPred)
#nbTestPred <- nbTestPred %>% select(ImageId, Label)
#write.csv(nbTestPred, 'Naive Bayes Classifier.csv', row.names = FALSE)
### end crossvalidation -- present results for all folds
(table(unlist(AllResults),unlist(AllLabels)))
#########Testing with Kaggle sample#########
#TestTotalDF_noLabel<-TestTotalDF[-c(1)]
#(head(TestTotalDF_noLabel))
#nb_Pred <- predict(train_naibayes, TestTotalDF_noLabel)
#nb_Pred
### Export naive Bayesbest result and run through Kaggle
#nbTestPred <- predict(train_naibayes,TestTotalDF, type = 'class')
#nbTestPred <- data.frame(nbTestPred)
#colnames(nbTestPred)[1] <- 'Label'
#nbTestPred$ImageId <- 1:nrow(nbTestPred)
#nbTestPred <- nbTestPred %>% select(ImageId, Label)
#write.csv(nbTestPred, 'Naive Bayes Classifier.csv', row.names = FALSE)
trainset <- read.csv("/Users/timtieng/Library/CloudStorage/OneDrive-Personal/Desktop/Masters in Applied Data Science/IST-707 Machine Learning/Homework 6 and 7/digit-recognizer/train.csv")
trainset$label <- factor(trainset$label)
#Create a random sample of n% of train data set
percent <- .15
dimReduce <- .10
set.seed(275)
DigitSplit <- sample(nrow(trainset),nrow(trainset)*percent)
trainset <- trainset[DigitSplit,]
dim(trainset)
# Setting static variables used throughout the Models section
N <- nrow(trainset)
kfolds <- 2
set.seed(30)
holdout <- split(sample(1:N), 1:kfolds)
# Function for model evaluation
get_accuracy_rate <- function(results_table, total_cases) {
diagonal_sum <- sum(c(results_table[[1]], results_table[[12]], results_table[[23]], results_table[[34]],
results_table[[45]], results_table[[56]], results_table[[67]], results_table[[78]],
results_table[[89]], results_table[[100]]))
(diagonal_sum / total_cases)*100
}
# Data Processing
# Discretizing at 87%
binarized_trainset <- trainset
for (col in colnames(binarized_trainset)) {
if (col != "label") {
binarized_trainset[, c(col)] <- ifelse(binarized_trainset[, c(col)] > 131, 1, 0)
}
}
for (col in colnames(binarized_trainset)) {
if (col != "label") {
binarized_trainset[, c(col)] <- as.factor(binarized_trainset[, c(col)])
}
}
digit_freq <- sqldf("SELECT label, COUNT(label) as count
FROM trainset
GROUP BY label")
ggplot(digit_freq, aes(x=reorder(label, -count), y=count)) + geom_bar(stat="identity") + xlab("Written Digit") + ylab("Frequency Count") + ggtitle("Written Digit by Frequency Count")
zero <- 0
fifty <- 0
one_hundred <- 0
one_hundred_fifty <- 0
two_hundred <- 0
two_hundred_fifty_five <- 0
for (col in colnames(trainset)) {
if (col != "label") {
#binarized_trainset[,c(col)] <- ifelse(binarized_trainset[,c(col)] > 131, 1, 0)
ifelse(trainset[,c(col)] == 0, zero <- zero + 1, ifelse(
trainset[,c(col)] < 51, fifty <- fifty + 1, ifelse(
trainset[,c(col)] < 101, one_hundred <- one_hundred + 1, ifelse(
trainset[,c(col)] < 151, one_hundred_fifty <- one_hundred_fifty + 1, ifelse(
trainset[,c(col)] < 201, two_hundred <- two_hundred + 1, two_hundred_fifty_five + 1
)
)
)
)
)
}
}
color_bins <- data.frame("color_bin"=c("0", "50", "100", "150", "200", "255"),
"count"=c(zero, fifty, one_hundred, one_hundred_fifty, two_hundred, two_hundred_fifty_five))
ggplot(color_bins, aes(x=reorder(color_bin, -count), y=count)) + geom_bar(stat="identity") + xlab("Color Bin") + ylab("Frequency Count") + ggtitle("Color Bin by Frequency Count")
color_freq <- data.frame("0"=c(), "1"=c())
for (col in colnames(binarized_trainset)) {
if (col != "label") {
zero <- c(length(which(binarized_trainset[,c(col)] == 0)))
one <- c(length(which(binarized_trainset[,c(col)] == 1)))
color_freq <- rbind(color_freq, data.frame("0"=zero, "1"=one))
}
}
colnames(color_freq) <- c("zero", "one")
color_freq <- data.frame("number"=c("zero", "one"), "count"=c(sum(color_freq$zero), sum(color_freq$one)))
ggplot(color_freq, aes(x=number, y=count)) + geom_bar(stat="identity") + xlab("Color Number") + ylab("Count") + ggtitle("Color Number by Count")
# K Means = 7
k_guess = 7# round(sqrt(nrow(trainset)))
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds) {
new_test <- trainset[holdout[[k]], ]
new_train <- trainset[-holdout[[k]], ]
new_test_no_label <- new_test[-c(1)]
new_test_just_label <- new_test[c(1)]
pred <- knn(train=new_train, test=new_test, cl=new_train$label, k=k_guess, prob=FALSE)
all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
# K Means = 3
k_guess = 3# round(sqrt(nrow(trainset)))
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds) {
new_test <- trainset[holdout[[k]], ]
new_train <- trainset[-holdout[[k]], ]
new_test_no_label <- new_test[-c(1)]
new_test_just_label <- new_test[c(1)]
pred <- knn(train=new_train, test=new_test, cl=new_train$label, k=k_guess, prob=FALSE)
all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
# K Means = 5
k_guess = 5# round(sqrt(nrow(trainset)))
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds) {
new_test <- trainset[holdout[[k]], ]
new_train <- trainset[-holdout[[k]], ]
new_test_no_label <- new_test[-c(1)]
new_test_just_label <- new_test[c(1)]
pred <- knn(train=new_train, test=new_test, cl=new_train$label, k=k_guess, prob=FALSE)
all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
# K Means = 8
k_guess = 8# round(sqrt(nrow(trainset)))
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds) {
new_test <- trainset[holdout[[k]], ]
new_train <- trainset[-holdout[[k]], ]
new_test_no_label <- new_test[-c(1)]
new_test_just_label <- new_test[c(1)]
pred <- knn(train=new_train, test=new_test, cl=new_train$label, k=k_guess, prob=FALSE)
all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
cols_to_remove = c()
for (col in colnames(trainset)) {
if (col != "label") {
if (length(unique(trainset[, c(col)])) == 1) {
cols_to_remove <- c(cols_to_remove, col)
}
}
}
svm_trainset <- trainset[-which(colnames(trainset) %in% cols_to_remove)]
# Baseline SVM - no changes to data
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds) {
new_test <- svm_trainset[holdout[[k]], ]
new_train <- svm_trainset[-holdout[[k]], ]
new_test_no_label <- new_test[-c(1)]
new_test_just_label <- new_test[c(1)]
test_model <- svm(label ~ ., new_train, na.action=na.pass)
pred <- predict(test_model, new_test_no_label, type=c("class"))
all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
# Binarizing preprocessed SVM trainset
binarized_svm_trainset <- svm_trainset
for (col in colnames(binarized_svm_trainset)) {
if (col != "label") {
binarized_svm_trainset[, c(col)] <- ifelse(binarized_svm_trainset[, c(col)] > 131, 1, 0)
}
}
for (col in colnames(binarized_svm_trainset)) {
if (col != "label") {
binarized_svm_trainset[, c(col)] <- as.factor(binarized_svm_trainset[, c(col)])
}
}
cols_to_remove = c()
for (col in colnames(binarized_svm_trainset)) {
if (col != "label") {
if (length(unique(binarized_svm_trainset[, c(col)])) == 1) {
cols_to_remove <- c(cols_to_remove, col)
}
}
}
binarized_svm_trainset <- binarized_svm_trainset[-which(colnames(binarized_svm_trainset) %in% cols_to_remove)]
# Testing SVM on new data
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds) {
new_test <- binarized_svm_trainset[holdout[[k]], ]
new_train <- binarized_svm_trainset[-holdout[[k]], ]
new_test_no_label <- new_test[-c(1)]
new_test_just_label <- new_test[c(1)]
test_model <- svm(label ~ ., new_train, na.action=na.pass)
pred <- predict(test_model, new_test_no_label, type=c("class"))
all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
# Did accurracy improve here? Which sets of parameters seem to work best for SVMs on this data set??? lets try some more.
# Polynomial Kernel
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds) {
new_test <- binarized_svm_trainset[holdout[[k]], ]
new_train <- binarized_svm_trainset[-holdout[[k]], ]
new_test_no_label <- new_test[-c(1)]
new_test_just_label <- new_test[c(1)]
test_model <- svm(label ~ ., new_train, kernel="polynomial", na.action=na.pass)
pred <- predict(test_model, new_test_no_label, type=c("class"))
all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
# Radial Kernel
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds) {
new_test <- binarized_svm_trainset[holdout[[k]], ]
new_train <- binarized_svm_trainset[-holdout[[k]], ]
new_test_no_label <- new_test[-c(1)]
new_test_just_label <- new_test[c(1)]
test_model <- svm(label ~ ., new_train, kernel="radial", na.action=na.pass)
pred <- predict(test_model, new_test_no_label, type=c("class"))
all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
# sigmoid Kernal
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds) {
new_test <- binarized_svm_trainset[holdout[[k]], ]
new_train <- binarized_svm_trainset[-holdout[[k]], ]
new_test_no_label <- new_test[-c(1)]
new_test_just_label <- new_test[c(1)]
test_model <- svm(label ~ ., new_train, kernel="sigmoid", na.action=na.pass)
pred <- predict(test_model, new_test_no_label, type=c("class"))
all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
